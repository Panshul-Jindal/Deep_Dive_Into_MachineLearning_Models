{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b06d4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5610 - EE2802 - AI2000 - AI5000) </b> </h1> </center>\n",
    "\n",
    "<b> Programming Assignment - 04 : Neural Networks </b>\n",
    "\n",
    "\n",
    "This programming assignment gives you a chance to perform the classification task using neural networks. You will get to build a neural network from scratch and train and test it on a standard classification dataset. Further you will learn different tricks and techniques to train a neural network eficiently by observing few important issues and trying to overcome them. This includes observing the performance of the network for different activation functions and optimization algorithms. We will conclude with implementation of various regularization techniques to overcome the problems of overfitting and vanishing gradients.\n",
    "\n",
    "<b> Instructions </b>\n",
    "1. Plagiarism is strictly prohibited.\n",
    "2. Delayed submissions will be penalized with a scaling factor of 0.5 per day.\n",
    "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d89af6",
   "metadata": {},
   "source": [
    "### Basically in Glance\n",
    "\n",
    "- NN Based Classification from Scratch\n",
    "- Understanding various Activation Functions\n",
    "- Understanding Optimization Algorithms\n",
    "- Understanding Regularization Methods\n",
    "- Comparision with Linear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b4ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All imports\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b7ff2",
   "metadata": {},
   "source": [
    "#  Part 1 🧠✨ Neural Network Based Digit Classification — MNIST 🖼️🔢\n",
    "\n",
    "This programming assignment focuses on building a **Feedforward Neural Network** from scratch to classify handwritten digits using the **MNIST dataset**. The dataset consists of grayscale images of size **28×28 pixels**, each representing a digit from **0 to 9**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ 1: Load MNIST Data & Create Train-Test Splits\n",
    "\n",
    "📌 **Instructions:**\n",
    "- The MNIST dataset contains **70,000 images** in total.\n",
    "- Split the dataset into:\n",
    "  - ✅ **60,000** images for training  \n",
    "  - 🧪 **10,000** images for testing  \n",
    "- ✅ The code for downloading and splitting the dataset is provided.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️  2: Design a Feedforward Classification Network\n",
    "\n",
    "We will build a **3-layer Feedforward Neural Network** with the following architecture:\n",
    "\n",
    "📐 **Network Dimensions:**\n",
    "- Input: 784 (28 × 28 flattened)\n",
    "- Hidden Layer 1: 512 nodes\n",
    "- Hidden Layer 2: 512 nodes\n",
    "- Output Layer: 10 nodes (one for each digit class)\n",
    "\n",
    "🧮 **Mathematical Representation:**\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = h(\\mathbf{W}_3 \\cdot g(\\mathbf{W}_2 \\cdot g(\\mathbf{W}_1 \\cdot \\mathbf{x})))\n",
    "$$\n",
    "\n",
    "- $ \\mathbf{W}_1 \\in \\mathbb{R}^{512 \\times 784} $\n",
    "- $ \\mathbf{W}_2 \\in \\mathbb{R}^{512 \\times 512} $\n",
    "- $ \\mathbf{W}_3 \\in \\mathbb{R}^{10 \\times 512} $\n",
    "\n",
    "✨ **Activations:**\n",
    "- Hidden layers: `ReLU` ⚡  \n",
    "- Output layer: `Softmax` 🎯\n",
    "\n",
    "---\n",
    "\n",
    "## 🏋️  3: Train the Neural Network\n",
    "\n",
    "### 🔄 Steps:\n",
    "1. 📦 **Flatten** 28×28 images into 784-dimensional vectors.  \n",
    "2. 🔁 **Randomly initialize** the weights $ \\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{W}_3 $  \n",
    "3. 🧠 **Feedforward pass** to compute class posteriors.  \n",
    "4. 📉 **Compute loss** between predicted probabilities and true labels (cross-entropy loss suggested).  \n",
    "5. 🔧 **Backpropagate** the loss to compute gradients.  \n",
    "6. ⚙️ **Update parameters** using **Stochastic Gradient Descent (SGD)**.  \n",
    "7. 🧪 **Tune hyperparameters** such as learning rate, batch size, and epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 4: Evaluate the Model\n",
    "\n",
    "### ✅ Steps:\n",
    "- 🔁 Run a **feedforward pass** on the test data.  \n",
    "- 🧠 **Predict the class** with the highest posterior probability.  \n",
    "- 📉 **Compute loss** and **accuracy**.  \n",
    "- 📋 **Report observations**, performance metrics, and any interesting behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Bonus Tips:\n",
    "- Try visualizing some predictions 📸  \n",
    "- Experiment with learning rates and see how they affect training!  \n",
    "- Track loss/accuracy over epochs using plots 📈\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976599f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data (60000, 28, 28)\n",
      "Training labels (60000,)\n",
      "Testing data (10000, 28, 28)\n",
      "Testing labels (10000,)\n"
     ]
    }
   ],
   "source": [
    "##1 Load data and create train test splits\n",
    " ##################################################\n",
    "#Load MNIST data.\n",
    "##################################################\n",
    "import torchvision.datasets as datasets\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "#Training data\n",
    "mnist_traindata = mnist_trainset.data.numpy()\n",
    "mnist_trainlabel = mnist_trainset.targets.numpy()\n",
    "print(\"Training data\",mnist_traindata.shape)\n",
    "print(\"Training labels\",mnist_trainlabel.shape)\n",
    "\n",
    "#Testing data\n",
    "mnist_testdata = mnist_testset.data.numpy()\n",
    "mnist_testlabel = mnist_testset.targets.numpy()\n",
    "print(\"Testing data\",mnist_testdata.shape)\n",
    "print(\"Testing labels\",mnist_testlabel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0ca943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 1 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 1 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 2 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 2 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 3 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 3 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 4 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 4 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 5 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 5 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 6 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 6 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 7 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 7 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 8 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 8 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 9 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 9 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 10 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 10 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 11 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 11 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 12 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 12 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 13 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 13 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 14 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 14 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 15 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 15 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 16 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 16 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 17 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 17 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 18 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 18 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 19 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 19 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 20 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 20 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 21 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 21 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 22 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 22 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 23 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 23 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 24 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 24 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 25 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 25 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 26 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 26 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 27 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 27 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 28 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 28 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 29 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 29 | Val Acc: 16.67 | Val Error: 18.47\n",
      "Predictions [[1]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [9]]\n",
      "actual labels [[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [3]\n",
      " [6]\n",
      " [1]]\n",
      "Epoch 30 : Accuracy : 10.0 Error : 20.232949406118053\n",
      "Epoch 30 | Val Acc: 16.67 | Val Error: 18.47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##################################################\n",
    "#2 Define the architecture\n",
    "##################################################\n",
    "\n",
    "#Complete the below function to impliment ReLU activation function\n",
    "def ReLu(inp):\n",
    "  return np.maximum(inp,0)\n",
    "  \n",
    "\n",
    "#Complete the below function to impliment gradient of ReLU activation function\n",
    "def gradReLu(inp):\n",
    "    return (inp > 0).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def LeakyReLu(inp, alpha=0.01):\n",
    "  return np.maximum(inp, alpha * inp)\n",
    "  \n",
    "def gradLeakyReLu(inp, alpha=0.01):\n",
    "  return np.where(inp > 0, 1.0, alpha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "#Complete the below function to impliment softmax activation function\n",
    "# def softmax(X):\n",
    "#     X = X - np.max(X, axis=0, keepdims=True)  # keepdims is key here\n",
    "#     exp_X = np.exp(X)\n",
    "#     return exp_X / np.sum(exp_X, axis=0, keepdims=True)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "#Complete the below function to impliment forward propagation of data\n",
    "def fwdPropagate(inputs, weights,bias):\n",
    "  #Inputs: input data, paramters of network\n",
    "  W1, W2, W3 = weights\n",
    "  b1,b2,b3 = bias\n",
    "\n",
    "  A1 = W1 @ inputs + b1\n",
    "  # print(A1.shape)\n",
    "  Z1 = LeakyReLu(A1)\n",
    "  A2 = W2 @ Z1 + b2\n",
    "  Z2 = LeakyReLu(A2)\n",
    "  A3 = W3 @ Z2 + b3\n",
    "  outps = softmax(A3)\n",
    "\n",
    "\n",
    "\n",
    "  #Return the requires outputs, i.e., final output and intermediate activations\n",
    "  return [A1,A2,A3],[inputs,Z1,Z2,outps]\n",
    "\n",
    "\n",
    "#Complete the below function to compute the gradients\n",
    "# def computeGradients(inputs, targets, weights, activations):\n",
    "#   #Inputs: input data, targets, parameters of netwrok, intermediate activations\n",
    "\n",
    "#   #Compute the loss\n",
    "\n",
    "\n",
    "\n",
    "#   #Compote the derivative of loss at parameters\n",
    "\n",
    "#   #Return the gradients\n",
    "#   return [dj_dw1, dj_dw2, dj_dw3]\n",
    "\n",
    "#Complete the below function to update the parameters using the above computed gradients\n",
    "def applyGradients(weights,biases, gradients_weights, gradient_biases, learning_rate):\n",
    "  #Inputs: weights, gradients, and learning rate\n",
    "  W1, W2, W3 = weights\n",
    "  b1,b2,b3 = biases\n",
    "  nabla_W1, nabla_W2, nabla_W3 = gradients_weights\n",
    "  nabla_b1,nabla_b2,nabla_b3 = gradient_biases\n",
    "\n",
    "  # print(\"W1 Shape\", W1.shape)\n",
    "  # print(\"nabla_W1 shape\", nabla_W1.shape)\n",
    "  # for i in range(3):\n",
    "  #   print(\"Weights shape\", weights[i].shape , \"nabla_Weight shape: \",gradients_weights[i].shape )\n",
    "\n",
    "  W1 = W1 - learning_rate * nabla_W1\n",
    "  W2 = W2 - learning_rate * nabla_W2\n",
    "  W3 = W3 - learning_rate * nabla_W3\n",
    "\n",
    "  b1= b1 - learning_rate * nabla_b1\n",
    "  b2 = b2 - learning_rate * nabla_b2\n",
    "  b3 = b3 - learning_rate * nabla_b3\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  #Return the updated parameters\n",
    "  return [W1, W2, W3],[b1,b2,b3]\n",
    "\n",
    "#Complete the below function to complete the backpropagation ste\n",
    "def backPropagate( targets, weights,biases, activations,neuronal_outputs,learning_rate):\n",
    "  #Inputs: input data, targets, parameters of network, intermediate activations, learning rate of optimization algorithm\n",
    "\n",
    "  # nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "  # nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "  W1,W2,W3 = weights\n",
    "  b1,b2,b3 = biases\n",
    "  m = activations[0].shape[1]\n",
    "\n",
    "  X,Z1,Z2,Z3 = neuronal_outputs\n",
    "  A1,A2,A3 = activations\n",
    "\n",
    "  # # print(\"Activation shape\")\n",
    "  # # for activation in activations:\n",
    "  # #   print(activation.shape)\n",
    "\n",
    "  # # print(\"Neuronal Outputs\")\n",
    "  # # for w in neuronal_outputs:\n",
    "  # #   print(w.shape)\n",
    "  \n",
    "  # delta = neuronal_outputs[-1] - targets\n",
    "\n",
    "  # nabla_b[-1] = np.sum(delta,axis =1 )[:,np.newaxis]\n",
    "  # nabla_w[-1] = delta @ neuronal_outputs[-2].T\n",
    "  # # print(f\"Layer {3} Activation shape {activations[-1].shape}  delta shape {delta.shape} nabla_w shape  {nabla_w[-1].shape}\")\n",
    "\n",
    "  # layer_count = 3\n",
    "  # for l in range(2,layer_count+1):\n",
    "  #   a = activations[-l]\n",
    "  #   h_prime = gradLeakyReLu(a)\n",
    "\n",
    "  #   delta = weights[-l +1].T @ delta  * h_prime\n",
    "  #   col_min = np.min(delta, axis=0)  # shape: (N,)\n",
    "  #   col_max = np.max(delta, axis=0)  # shape: (N,)    # print(f\"Layer {3-l} Activation shape {a.shape}  delta shape {delta.shape}\")\n",
    "\n",
    "  #   print(\"Min per sample:\", col_min[:10])\n",
    "  #   print(\"Max per sample:\", col_max[:10])\n",
    "\n",
    "  #   nabla_b[-l] = np.sum(delta,axis =1)[:,np.newaxis]\n",
    "\n",
    "  #   nabla_w[-l] = delta @ neuronal_outputs[-l-1].T\n",
    "\n",
    "  delta3 = Z3 - targets\n",
    "  nabla_w3 = 1/m * delta3 @ Z2.T\n",
    "  nabla_b3 = 1/m * np.sum(delta3)\n",
    "\n",
    "  delta2 = W3.T @delta3  * gradReLu(A2)\n",
    "  nabla_w2  = 1/m * delta2@ Z1.T\n",
    "  nabla_b2 = 1/m * np.sum(delta2)\n",
    "\n",
    "\n",
    "  delta1 = W2.T @delta2  * gradReLu(A1)\n",
    "  nabla_w1  = 1/m * delta1@ X.T\n",
    "  nabla_b1 = 1/m * np.sum(delta1)\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  return applyGradients(weights,biases,[nabla_w1,nabla_w2,nabla_w3],[nabla_b1,nabla_b2,nabla_b3],learning_rate)\n",
    "\n",
    "\n",
    "def error(predicted, true):\n",
    "    eps = 1e-10\n",
    "    return -np.sum(true * np.log(predicted + eps)) / predicted.shape[1]\n",
    "\n",
    "def accuracy(t,t_hat):\n",
    "    return np.sum(t==t_hat)/t.size *100\n",
    "      \n",
    "\n",
    "\n",
    "##################################################\n",
    "#3 Train the network\n",
    "##################################################\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "#Complete the below function to complete the training of network\n",
    "def training(inputs, targets_idx, batch_size = 128, epochs=30, train_val_split=0.8, learning_rate=0.01):\n",
    "\n",
    "  #Set the hyperparameters\n",
    "  hidden_units = [512,512]\n",
    "  n_classes = 10\n",
    "  n_samples = inputs.shape[1]\n",
    "\n",
    "\n",
    "  sizes = [inputs.shape[0],512,512,n_classes]\n",
    "\n",
    "  #Split the training data into two parts.\n",
    "  #Use 90 percent of training data for training the network.\n",
    "  #Remaining 10 percent as validation data\n",
    "\n",
    "  train_samples = int(train_val_split * n_samples) \n",
    "  # validation_samples = n_samples - train_samples\n",
    "\n",
    "\n",
    "\n",
    "  permutation = np.random.permutation(n_samples)\n",
    "\n",
    "  # Apply the permutation to both inputs and labels\n",
    "  shuffled_inputs = inputs[:, permutation]\n",
    "  shuffled_labels = targets_idx[:, permutation]\n",
    "\n",
    "  # Split based on shuffled data\n",
    "  train_X = shuffled_inputs[:, :train_samples]\n",
    "  train_idx = shuffled_labels[:, :train_samples]\n",
    "\n",
    "  validate_X = shuffled_inputs[:, train_samples:]\n",
    "  validate_idx = shuffled_labels[:, train_samples:]\n",
    "\n",
    "  \n",
    "  n_batches = int(train_samples/batch_size)\n",
    "\n",
    "  # print(\"Train_X shape\",train_X.shape)\n",
    "  # print(\"Train_idx shape\",train_idx.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #Randomly initialize the weights\n",
    "  biases = [np.random.randn(y,1) *0.01 for y in sizes[1:]]\n",
    "\n",
    "\n",
    "  weights = [np.random.randn(y, x) * np.sqrt(2. / x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "  # batch =3\n",
    "  # batch_X = train_X[:,batch* batch_size : (batch+1 )* batch_size]\n",
    "  # batch_idx = targets_idx[:,batch * batch_size : (batch+1) * batch_size]\n",
    "\n",
    "\n",
    "\n",
    "  # print(\"Batch_X shape\",batch_X.shape)\n",
    "  # print(\"Batch_idx shape\",batch_idx.shape)\n",
    "\n",
    "\n",
    "\n",
    "  #Interate for epochs times\n",
    "  for epoch in range(epochs):\n",
    "    #Shuffle the training data\n",
    "\n",
    "    #Interate through the batches of data\n",
    "    for batch in range(n_batches):\n",
    "      #Get the batch of data\n",
    "      batch_X = train_X[:,batch* batch_size : (batch+1 )* batch_size]\n",
    "      batch_idx = targets_idx[:,batch * batch_size : (batch+1) * batch_size]\n",
    "\n",
    "      #Forward propagation\n",
    "      activations,neuronal_outputs = fwdPropagate(batch_X, weights,biases)\n",
    "\n",
    "      #Backward propagation\n",
    "      weights,biases = backPropagate(batch_idx,weights,biases, activations,neuronal_outputs, learning_rate)\n",
    "\n",
    "      # active_neurons = np.mean(activations > 0)\n",
    "      # print(f\"Active neurons: {active_neurons*100:.2f}%\")\n",
    "\n",
    "      # print(f\"Batch {batch} Exectued successfully Updated Weights and Biases \")\n",
    "\n",
    "\n",
    "\n",
    "    #Compute outpus on trianing data\n",
    "    activations,neuronal_outputs = fwdPropagate(inputs,weights,biases)\n",
    "    outputs = neuronal_outputs[-1]\n",
    "\n",
    "    \n",
    "    predictions = np.argmax(outputs,axis = 0)[:,np.newaxis]\n",
    "    actual_labels = np.argmax(targets_idx,axis = 0)[:,np.newaxis]\n",
    "    print(\"Predictions\",predictions[:15])\n",
    "    print(\"actual labels\", actual_labels[:15])\n",
    "    #Compute training accuracy, and training error\n",
    "    train_error = error(outputs,targets_idx)\n",
    "    train_accuracy = accuracy(predictions,actual_labels)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} : Accuracy : {train_accuracy} Error : {train_error}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Compute outputs on validation data\n",
    "    val_activations, val_outputs = fwdPropagate(validate_X, weights, biases)\n",
    "    val_preds = np.argmax(val_outputs[-1], axis=0)[:, np.newaxis]\n",
    "    val_labels = np.argmax(validate_idx, axis=0)[:, np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "    #Compute validation accuracy, and validation error\n",
    "    val_acc = accuracy(val_preds, val_labels)\n",
    "    val_err = error(val_outputs[-1], validate_idx)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Val Acc: {val_acc:.2f} | Val Error: {val_err:.2f}\")\n",
    "\n",
    "    #Print the statistics of training, i.e., training error, training accuracy, validation error, and validation accuracy\n",
    "\n",
    "\n",
    "    #Save the parameters of network\n",
    "\n",
    "\n",
    "#Call the training function to train the network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_inputs=np.array([i.flatten() for i in mnist_traindata]).T[:,:150]\n",
    "train_labels= np.array([vectorized_result(j).flatten() for j in mnist_trainlabel]).T[:,:150]\n",
    "\n",
    "training(train_inputs,train_labels,learning_rate =0.0001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af90e840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= np.array([1,2,3])\n",
    "exp_x = np.e**x\n",
    "exp_x/  (np.sum(exp_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d577cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04742587, 0.26894142, 0.99752738],\n",
       "       [0.95257413, 0.73105858, 0.00247262]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "X= np.array([\n",
    "  [1,4,5],\n",
    "  [4,5,-1]\n",
    "])\n",
    "\n",
    "softmax(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245f608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c3206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 784\n",
      "512 512\n",
      "10 512\n"
     ]
    }
   ],
   "source": [
    "sizes = [784,512,512,10]\n",
    "for x,y in zip(sizes[:-1],sizes[1:]):\n",
    "    print(y,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283d7a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/__init__.py:320\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "np.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a91848",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#4 Evaluate the performance on test data\n",
    "##################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f495f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [-1,2,3],\n",
    "    [4,5,6]\n",
    "])\n",
    "\n",
    "def gradReLu(inp):\n",
    "  if inp >=0:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.apply_along_axis(gradReLu_vec,0,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade935b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 🔍🧠 Part 2: Understanding Activation Functions ⚡📉\n",
    "\n",
    "In this part, you will explore how different **activation functions** affect the performance of a feedforward neural network trained on the **MNIST digit classification** task.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 1. Experiment with Activation Functions\n",
    "\n",
    "Train the same network architecture as in Part 1 using **different activation functions** in the hidden layers:\n",
    "\n",
    "🔁 **Activation Functions to try:**\n",
    "- 🌀 `Sigmoid`\n",
    "- 🔄 `Tanh`\n",
    "- ⚡ `ReLU`\n",
    "- ⚡💧 `Leaky ReLU`\n",
    "\n",
    "📌 Keep the following fixed:\n",
    "- Use **Stochastic Gradient Descent (SGD)** as the optimization algorithm\n",
    "- Keep the rest of the architecture, loss function, and learning setup same\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 2. Report & Compare Performance\n",
    "\n",
    "🧠 **Evaluation Instructions:**\n",
    "- Run the trained model on the **MNIST test dataset**\n",
    "- Record and report the **accuracy** for each activation function\n",
    "- ✍️ Write down your **observations** in the report:\n",
    "  - How does the choice of activation affect learning?\n",
    "  - Which function performed best and why?\n",
    "  - Are there any signs of vanishing gradients or training instability?\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Sample Table for Results (fill this in later):\n",
    "\n",
    "| Activation Function | Test Accuracy (%) | Observations |\n",
    "|---------------------|-------------------|--------------|\n",
    "| Sigmoid             |                   |              |\n",
    "| Tanh                |                   |              |\n",
    "| ReLU                |                   |              |\n",
    "| Leaky ReLU          |                   |              |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Tips:\n",
    "- You may visualize training loss curves for each activation function 📉\n",
    "- Consider running multiple trials to average out randomness\n",
    "- Look at confusion matrices to understand misclassification patterns 🔍\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584c3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4565605",
   "metadata": {},
   "source": [
    "# ⚙️📈 Part 3: Understanding Optimization Algorithms 🧠🔧\n",
    "\n",
    "In this part, you'll explore how different **optimization algorithms** affect the training performance of your classification network.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 1. Use the Best Activation Function\n",
    "\n",
    "- Choose the **best-performing activation function** from your experiments in **Part 2** (e.g., ReLU or Tanh).\n",
    "- Use this activation function in your network’s hidden layers.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 2. Train with Adam Optimizer\n",
    "\n",
    "Train the same classification network using the **Adam optimization algorithm**, instead of SGD.\n",
    "\n",
    "🔁 Keep the rest of the setup **unchanged**:\n",
    "- Same architecture (3-layer FFNN)\n",
    "- Same initialization\n",
    "- Same learning rate (or tune slightly if necessary)\n",
    "- Same loss function (Cross-Entropy)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ 3. Compare Performance: SGD vs Adam\n",
    "\n",
    "Evaluate and compare the models trained using:\n",
    "- 🔁 **Stochastic Gradient Descent (SGD)**\n",
    "- ⚙️ **Adam Optimizer**\n",
    "\n",
    "🧪 **Metrics to Compare:**\n",
    "- Test Accuracy\n",
    "- Convergence speed (training loss curves)\n",
    "- Stability of training (variance in accuracy across epochs)\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Sample Comparison Table:\n",
    "\n",
    "| Optimizer | Activation Function | Test Accuracy (%) | Observations |\n",
    "|-----------|---------------------|-------------------|--------------|\n",
    "| SGD       | ReLU (or best)      |                   |              |\n",
    "| Adam      | ReLU (or best)      |                   |              |\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ 4. Report Your Observations\n",
    "\n",
    "🔍 **What to observe:**\n",
    "- Did Adam improve the accuracy?\n",
    "- Was convergence faster or slower than SGD?\n",
    "- Any trade-offs noticed (e.g., stability vs. generalization)?\n",
    "\n",
    "🗒️ Document your insights in the report.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Tips:\n",
    "- Plot accuracy/loss over epochs for both optimizers\n",
    "- Try using same learning rates for fairness, or optimize them slightly for best results\n",
    "- Optionally try RMSprop, Adagrad, etc., for deeper exploration 🔍\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9021fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa95b91",
   "metadata": {},
   "source": [
    "# 🧪🧰 Part 4: Understanding Regularization Methods 🔒📉\n",
    "\n",
    "In this part of the assignment, you'll explore **regularization techniques** to reduce **overfitting** in your neural network. You'll use the network from previous parts and incorporate different methods to improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Goal:\n",
    "Retrain the neural network using the following **regularization techniques**, and compare their performance on the **MNIST test set**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 1. Weight Regularization (L2 Penalty)\n",
    "\n",
    "- Add an L2 regularization term to the loss function:\n",
    "  \n",
    "  $$\n",
    "  \\text{Loss}_{\\text{total}} = \\text{CrossEntropyLoss} + \\lambda \\left( \\lVert \\mathbf{W}_1 \\rVert^2 + \\lVert \\mathbf{W}_2 \\rVert^2 + \\lVert \\mathbf{W}_3 \\rVert^2 \\right)\n",
    "  $$\n",
    "\n",
    "- Experiment with different values of **$\\lambda$** (e.g., `0.001`, `0.01`, `0.1`) and report the impact.\n",
    "\n",
    "---\n",
    "\n",
    "## 💧 2. Dropout Regularization\n",
    "\n",
    "- Introduce **Dropout** in the hidden layers with a probability of `0.2`.\n",
    "- Dropout randomly disables some neurons during training to prevent co-adaptation.\n",
    "- 🧪 **Important:** Disable Dropout during inference/evaluation.\n",
    "\n",
    "### 🔁 Suggestions:\n",
    "- Try with different probabilities: `0.1`, `0.2`, `0.5`\n",
    "- Compare their effects on validation accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## ⏹️ 3. Early Stopping\n",
    "\n",
    "- Monitor **validation loss** during training\n",
    "- Stop training when validation loss starts increasing, even if training loss is decreasing\n",
    "- Prevents the model from overfitting the training data\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Results Table\n",
    "\n",
    "| Regularization Method | Accuracy on Test Set (%) | Observations |\n",
    "|------------------------|--------------------------|--------------|\n",
    "| L2 (λ = 0.001)         |                          |              |\n",
    "| L2 (λ = 0.01)          |                          |              |\n",
    "| Dropout (p = 0.2)      |                          |              |\n",
    "| Dropout (p = 0.5)      |                          |              |\n",
    "| Early Stopping         |                          |              |\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Report Your Observations\n",
    "\n",
    "🧠 Consider reflecting on:\n",
    "- Which regularization method gave the best test accuracy?\n",
    "- Did any method significantly reduce overfitting?\n",
    "- How did training time, convergence, and stability change with each method?\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Tips:\n",
    "- You can visualize **training vs. validation loss curves** to better understand overfitting and the effect of early stopping.\n",
    "- For Dropout, visualize training accuracy vs test accuracy to observe generalization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cabc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60145f37",
   "metadata": {},
   "source": [
    "# ⚖️📊 Part 5: Comparison with Linear Classifiers 🤖 vs 📉\n",
    "\n",
    "In this final part of the assignment, you'll compare the classification performance of **deep neural networks** and **linear classifiers** on both linearly and non-linearly separable datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. Linearly Separable Data Generation\n",
    "\n",
    "📈 **Class 1**:\n",
    "- Gaussian distribution\n",
    "- Mean: $ \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $\n",
    "- Covariance: $ \\begin{bmatrix} 0.3 & 0.0 \\\\ 0.0 & 0.3 \\end{bmatrix} $\n",
    "\n",
    "📉 **Class 2**:\n",
    "- Gaussian distribution\n",
    "- Mean: $ \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} $\n",
    "- Covariance: $ \\begin{bmatrix} 0.3 & 0.0 \\\\ 0.0 & 0.3 \\end{bmatrix} $\n",
    "\n",
    "📦 **Split:**\n",
    "- 4500 samples per class for **training**\n",
    "- 500 samples per class for **testing**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. Non-linearly Separable Data\n",
    "\n",
    "- Predefined code provides:\n",
    "  - `class1_data` and `class2_data`\n",
    "  - ~5000 points per class\n",
    "\n",
    "📦 **Split:**\n",
    "- Use **90% for training**, **10% for testing**\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 Programming Tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 3. Linear Classification Models - Logistic Regression\n",
    "\n",
    "📘 **Model:**\n",
    "- $ y = \\frac{1}{1 + e^{-\\mathbf{w}^T \\mathbf{x}}} $\n",
    "\n",
    "### ✅ Your tasks:\n",
    "\n",
    "**a) Build a function `Logistic_Regression(X_train, Y_train, X_test)`**:\n",
    "- Initialize $ \\mathbf{w} $ randomly\n",
    "- Optimize using **iterative reweighted least squares**\n",
    "- Predict using learned $ \\mathbf{w} $\n",
    "\n",
    "**b) Evaluate test accuracy**  \n",
    "**c) Visualize decision regions**:\n",
    "- Use boundary plots or color-coded regions\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 4. Deep Neural Network-Based Classification\n",
    "\n",
    "🏗 **Architecture:**\n",
    "\n",
    "$ \\mathbf{y} = h(\\mathbf{W}_3 \\cdot g(\\mathbf{W}_2 \\cdot g(\\mathbf{W}_1 \\cdot \\mathbf{x}))) $\n",
    "\n",
    "### 🧱 Weight Dimensions:\n",
    "- $ \\mathbf{W}_1 \\in \\mathbb{R}^{3 \\times 2} $\n",
    "- $ \\mathbf{W}_2 \\in \\mathbb{R}^{3 \\times 3} $\n",
    "- $ \\mathbf{W}_3 \\in \\mathbb{R}^{1 \\times 3} $\n",
    "\n",
    "🧩 **Activation Functions**:\n",
    "- $g(.)$ = ReLU (hidden layers)\n",
    "- $h(.)$ = Sigmoid (output layer)\n",
    "\n",
    "🔁 **Posterior Probabilities**:\n",
    "- Class 1: $ \\sigma(z) $\n",
    "- Class 2: $ 1 - \\sigma(z) $\n",
    "\n",
    "### ✅ Your tasks:\n",
    "\n",
    "- Train the network on both datasets\n",
    "- Plot **second layer activation potentials**:\n",
    "  - Input entire data\n",
    "  - Visualize 3D hidden representation\n",
    "- Evaluate performance on test set\n",
    "- Comment on how non-linearity transforms the space\n",
    "\n",
    "---\n",
    "\n",
    "## 🆚 5. Comparison and Observations\n",
    "\n",
    "📊 Compare:\n",
    "- **Logistic Regression (Linear Model)**\n",
    "- **Feedforward Neural Network**\n",
    "\n",
    "### Suggested Comparison Table:\n",
    "\n",
    "| Dataset Type         | Classifier            | Test Accuracy (%) | Comments |\n",
    "|----------------------|------------------------|-------------------|----------|\n",
    "| Linearly Separable   | Logistic Regression    |                   |          |\n",
    "| Linearly Separable   | Neural Network         |                   |          |\n",
    "| Non-linearly Separable | Logistic Regression  |                   |          |\n",
    "| Non-linearly Separable | Neural Network       |                   |          |\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Final Observations\n",
    "\n",
    "- Which model performs better on linearly separable data?\n",
    "- Which model excels on non-linear data?\n",
    "- How do hidden activations help in class separation?\n",
    "- Is the added complexity of neural networks justified?\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Tips:\n",
    "- Use meshgrid and contour plots for decision boundaries\n",
    "- For 3D visualization of hidden layers, use `matplotlib`'s `Axes3D`\n",
    "- Observe the **power of feature transformation** done by neural networks!\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
